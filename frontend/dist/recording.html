<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recording...</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: transparent;
            overflow: hidden;
            display: flex;
            justify-content: center;
            align-items: center;
            width: 100vw;
            height: 100vh;
        }

        .container {
            background: #1a1a1a;
            border-radius: 16px;
            padding: 16px 20px;
            width: 616px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.9);
            border: 1px solid #2a2a2a;
        }

        .top-section {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 12px;
        }

        .recording-indicator {
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .dot {
            width: 8px;
            height: 8px;
            background: #3b82f6;
            border-radius: 50%;
            animation: pulse 1.5s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; transform: scale(1); }
            50% { opacity: 0.5; transform: scale(0.9); }
        }

        .model-selector {
            flex: 1;
        }

        .model-dropdown {
            background: transparent;
            border: 1px solid #3a3a3a;
            border-radius: 6px;
            padding: 6px 10px;
            color: #999;
            font-size: 12px;
            cursor: pointer;
            width: auto;
            max-width: 180px;
            outline: none;
            transition: all 0.2s;
        }

        .model-dropdown:hover {
            border-color: #4a4a4a;
            color: #bbb;
        }

        .model-dropdown:focus {
            border-color: #3b82f6;
        }

        .status-text {
            color: #666;
            font-size: 11px;
            margin-left: auto;
        }

        .visualizer-container {
            background: transparent;
            border-radius: 8px;
            padding: 12px 0;
            margin-bottom: 12px;
            height: 60px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 2px;
        }

        .visualizer-bar {
            width: 3px;
            background: #3b82f6;
            border-radius: 1.5px;
            transition: height 0.08s ease;
            height: 3px;
            min-height: 3px;
        }

        .loading .visualizer-bar {
            animation: wave 1.2s ease-in-out infinite;
        }

        @keyframes wave {
            0%, 100% { height: 3px; }
            50% { height: 30px; }
        }

        .buttons {
            display: flex;
            gap: 16px;
            justify-content: flex-end;
        }

        button {
            padding: 6px 16px;
            border: none;
            border-radius: 6px;
            font-size: 11px;
            font-weight: 400;
            cursor: pointer;
            transition: all 0.2s;
            background: transparent;
            color: #666;
        }

        button:hover {
            color: #999;
        }

        .stop-btn {
            border: 1px solid #3a3a3a;
        }

        .stop-btn:hover {
            border-color: #4a4a4a;
        }

        .cancel-btn {
            border: 1px solid #2a2a2a;
        }

        .cancel-btn:hover {
            border-color: #3a3a3a;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="top-section">
            <div class="recording-indicator">
                <div class="dot"></div>
            </div>
            <div class="model-selector">
                <select class="model-dropdown" id="modelSelect">
                    <option value="tiny">Tiny</option>
                    <option value="base">Base</option>
                    <option value="small" selected>Small</option>
                    <option value="medium">Medium</option>
                    <option value="large-v3">Large</option>
                </select>
            </div>
            <div class="status-text" id="statusText">Recording...</div>
        </div>
        <div class="visualizer-container" id="visualizer">
            <!-- Bars will be created dynamically -->
        </div>
        <div class="buttons">
            <button class="stop-btn" onclick="stopRecording()">Stop   F9</button>
            <button class="cancel-btn" onclick="cancelRecording()">Cancel   Esc</button>
        </div>
        
        <!-- Debug panel (only visible in console) -->
        <div style="display: none;">
            <button onclick="debugBackendAudio()" id="debugBtn">üîç Debug Backend</button>
            <button onclick="debugWebRTCAudio()" id="webrtcBtn">üé§ Test WebRTC</button>
        </div>
    </div>

    <script>
        const BACKEND_URL = 'http://127.0.0.1:8000';
        let bars = [];
        let audioLevelInterval = null;
        let waveAnimationInterval = null;
        
        // State management
        let currentState = 'idle'; // 'idle', 'recording', 'processing'
        let isRecording = false;
        
        // Audio visualizer implementation - completely rewritten
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let dataArray = null;
        let useWebRTC = false; // Fallback method

        // Initialize visualizer bars
        function initVisualizer() {
            const visualizer = document.getElementById('visualizer');
            const barCount = 80;  // Reduced for narrower window

            for (let i = 0; i < barCount; i++) {
                const bar = document.createElement('div');
                bar.className = 'visualizer-bar';
                visualizer.appendChild(bar);
                bars.push(bar);
            }
        }

        // NEW: Test backend audio level endpoint
        async function testBackendAudioLevel() {
            console.log('üîç Testing backend audio level endpoint...');
            try {
                const response = await fetch(`${BACKEND_URL}/audio_level`);
                if (response.ok) {
                    const data = await response.json();
                    console.log('üìä Backend audio level response:', data);
                    return data;
                } else {
                    console.error('‚ùå Backend audio level failed:', response.status);
                    return null;
                }
            } catch (error) {
                console.error('‚ùå Backend audio level error:', error);
                return null;
            }
        }

        // NEW: Initialize WebRTC audio visualizer (fallback)
        async function initWebRTCAudio() {
            console.log('üé§ Initializing WebRTC audio...');
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: false,
                        noiseSuppression: false,
                        autoGainControl: false
                    } 
                });
                
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                
                analyser.fftSize = 256;
                analyser.smoothingTimeConstant = 0.8;
                dataArray = new Uint8Array(analyser.frequencyBinCount);
                
                microphone.connect(analyser);
                
                console.log('‚úÖ WebRTC audio initialized');
                return true;
            } catch (error) {
                console.error('‚ùå WebRTC audio failed:', error);
                return false;
            }
        }

        // NEW: Get audio level from WebRTC
        function getWebRTCAudioLevel() {
            if (!analyser || !dataArray) return 0;
            
            analyser.getByteFrequencyData(dataArray);
            
            // Calculate average amplitude
            let sum = 0;
            for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i];
            }
            const average = sum / dataArray.length;
            
            // Normalize to 0-1
            return average / 255;
        }

        // Poll backend for audio levels (improved)
        async function pollAudioLevels() {
            if (currentState !== 'recording') {
                return; // Only poll when recording
            }

            let level = 0;

            if (useWebRTC) {
                // Use WebRTC fallback
                level = getWebRTCAudioLevel();
                console.log(`üé§ WebRTC audio level: ${level.toFixed(3)}`);
            } else {
                // Try backend first
                try {
                    const response = await fetch(`${BACKEND_URL}/audio_level`);
                    if (response.ok) {
                        const data = await response.json();
                        level = data.level || 0;
                        
                        // Log occasionally to verify it's working
                        if (Math.random() < 0.1) { // 10% of the time for better debugging
                            console.log(`üéôÔ∏è Backend audio level: ${level.toFixed(3)}, recording: ${data.recording}, queue: ${data.queue_size}`);
                        }
                    } else {
                        console.warn('‚ö†Ô∏è Backend audio level failed, switching to WebRTC fallback');
                        useWebRTC = true;
                        await initWebRTCAudio();
                        return;
                    }
                } catch (error) {
                    console.warn('‚ö†Ô∏è Backend not responding, switching to WebRTC fallback');
                    useWebRTC = true;
                    await initWebRTCAudio();
                    return;
                }
            }
            
            updateVisualizerFromLevel(level);
        }

        // Update visualizer from audio level
        function updateVisualizerFromLevel(level) {
            bars.forEach((bar, index) => {
                // Create wave pattern based on level
                const offset = (index / bars.length) * Math.PI * 4;
                const wave = Math.sin(offset + Date.now() / 200) * 0.3 + 0.7;
                const height = Math.max(3, level * 40 * wave);
                bar.style.height = `${height}px`;
            });
        }

        // State management functions
        async function setState(newState) {
            console.log(`üîÑ State change: ${currentState} ‚Üí ${newState}`);
            currentState = newState;
            await updateUI();
        }

        async function updateUI() {
            // Clear all animations first
            if (audioLevelInterval) {
                clearInterval(audioLevelInterval);
                audioLevelInterval = null;
            }
            if (waveAnimationInterval) {
                clearInterval(waveAnimationInterval);
                waveAnimationInterval = null;
            }
            
            // Stop WebRTC if switching away from recording
            if (currentState !== 'recording' && audioContext) {
                try {
                    audioContext.close();
                    audioContext = null;
                    analyser = null;
                    microphone = null;
                    dataArray = null;
                    useWebRTC = false;
                    console.log('üîá WebRTC audio stopped');
                } catch (e) {
                    console.warn('WebRTC cleanup error:', e);
                }
            }

            switch (currentState) {
                case 'recording':
                    document.getElementById('statusText').textContent = 'Recording...';
                    await startAudioVisualizer();
                    break;
                case 'processing':
                    document.getElementById('statusText').textContent = 'Processing...';
                    startProcessingAnimation();
                    break;
                case 'idle':
                default:
                    document.getElementById('statusText').textContent = 'Ready...';
                    resetBars();
                    break;
            }
        }

        async function startAudioVisualizer() {
            console.log('üé§ Starting audio visualizer');
            
            // Test backend first
            const backendTest = await testBackendAudioLevel();
            if (backendTest && backendTest.recording === false) {
                console.log('üìä Backend is available but not recording yet, this is expected');
            } else if (!backendTest) {
                console.log('‚ö†Ô∏è Backend not available, will try WebRTC fallback when needed');
            }
            
            // Start polling
            audioLevelInterval = setInterval(pollAudioLevels, 100);
        }

        function startProcessingAnimation() {
            console.log('üåä Starting processing animation');
            let waveOffset = 0;
            waveAnimationInterval = setInterval(() => {
                if (currentState !== 'processing') {
                    clearInterval(waveAnimationInterval);
                    waveAnimationInterval = null;
                    return;
                }
                bars.forEach((bar, index) => {
                    const delay = (index / bars.length) * Math.PI * 2;
                    const height = 10 + Math.sin(waveOffset + delay) * 12 + 12;
                    bar.style.height = `${height}px`;
                });
                waveOffset += 0.15;
            }, 50);
        }

        function resetBars() {
            bars.forEach(bar => {
                bar.style.height = '3px';
            });
        }

        // Show processing state (called by Rust)
        function showProcessing() {
            console.log('üåä SHOWING PROCESSING ANIMATION (called by Rust)');
            setState('processing');
        }

        // Stop and transcribe
        async function stopRecording() {
            console.log('üõë Stop button clicked');
            setState('processing');

            let transcribedText = null;

            try {
                const response = await fetch(`${BACKEND_URL}/stop`, { method: 'POST' });
                const data = await response.json();

                if (data.status === 'success' && data.text) {
                    console.log('‚úÖ Transcription:', data.text);
                    transcribedText = data.text;
                }
            } catch (error) {
                console.error('‚ùå Stop error:', error);
            }

            // Hide window FIRST (to restore focus)
            const { webviewWindow, invoke } = window.__TAURI_INTERNALS__;
            await webviewWindow.getCurrent().hide();
            console.log('‚úÖ Window hidden');

            // Wait for focus to return to text field
            await new Promise(resolve => setTimeout(resolve, 150));

            // THEN inject text (always inject, clipboard setting controls saving)
            if (transcribedText) {
                try {
                    const saveToClipboard = await invoke('get_clipboard_paste');
                    console.log(`üìã Injecting text (save to clipboard: ${saveToClipboard})`);
                    await invoke('inject_text_directly', { 
                        text: transcribedText,
                        saveToClipboard: saveToClipboard 
                    });
                } catch (error) {
                    console.error('‚ùå Injection error:', error);
                }
            }
        }

        // Cancel recording
        async function cancelRecording() {
            console.log('‚ùå Cancel button clicked');

            try {
                await fetch(`${BACKEND_URL}/cancel`, { method: 'POST' });
            } catch (error) {
                console.error('‚ùå Cancel error:', error);
            }

            // Hide window
            const { webviewWindow } = window.__TAURI_INTERNALS__;
            await webviewWindow.getCurrent().hide();
            console.log('‚úÖ Window hidden');
        }

        // Model selection handler
        document.getElementById('modelSelect').addEventListener('change', async (e) => {
            const model = e.target.value;
            console.log('üìù Model selected:', model);

            // Send to backend via Tauri command
            const { invoke } = window.__TAURI_INTERNALS__;
            try {
                await invoke('set_model_and_device', {
                    model: model,
                    device: 'auto'
                });
                console.log('‚úÖ Model updated');
            } catch (error) {
                console.error('‚ùå Model update error:', error);
            }
        });

        // Escape key cancels
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape') {
                e.preventDefault();
                cancelRecording();
            }
        });

        // Reset to recording state
        function resetToRecording() {
            console.log('üîÑ Resetting to recording state');
            setState('recording');
        }

        // Debug functions (accessible from console)
        window.debugBackendAudio = async function() {
            console.log('üîç === BACKEND AUDIO DEBUG ===');
            
            // Test health endpoint
            try {
                const health = await fetch(`${BACKEND_URL}/health`);
                const healthData = await health.json();
                console.log('üè• Backend health:', healthData);
            } catch (e) {
                console.error('‚ùå Backend health failed:', e);
            }
            
            // Test audio level endpoint
            for (let i = 0; i < 5; i++) {
                try {
                    const response = await fetch(`${BACKEND_URL}/audio_level`);
                    const data = await response.json();
                    console.log(`üìä Audio level ${i+1}:`, data);
                } catch (e) {
                    console.error(`‚ùå Audio level ${i+1} failed:`, e);
                }
                await new Promise(resolve => setTimeout(resolve, 500));
            }
        };
        
        window.debugWebRTCAudio = async function() {
            console.log('üé§ === WEBRTC AUDIO DEBUG ===');
            
            const success = await initWebRTCAudio();
            if (success) {
                console.log('‚úÖ WebRTC initialized, testing for 5 seconds...');
                for (let i = 0; i < 50; i++) {
                    const level = getWebRTCAudioLevel();
                    if (i % 10 === 0) {
                        console.log(`üé§ WebRTC level ${i/10 + 1}:`, level.toFixed(3));
                    }
                    await new Promise(resolve => setTimeout(resolve, 100));
                }
            }
        };

        // Initialize on load
        initVisualizer();
        console.log('‚úÖ Recording window loaded');
        console.log('‚úÖ Visualizer initialized with', bars.length, 'bars');
        console.log('üîß Debug commands available:');
        console.log('   debugBackendAudio() - Test backend audio endpoint');
        console.log('   debugWebRTCAudio() - Test WebRTC audio fallback');

        // Track previous visibility state
        let wasVisible = false;
        let isFirstShow = true;

        // Poll visibility to start/stop monitoring
        setInterval(async () => {
            const { webviewWindow } = window.__TAURI_INTERNALS__;
            const currentWindow = webviewWindow.getCurrent();
            const isVisible = await currentWindow.isVisible();

            // Window just became visible
            if (isVisible && !wasVisible) {
                console.log('üì∫ Window became visible - resetting to recording state');
                if (isFirstShow) {
                    console.log('üåü First show detected');
                    isFirstShow = false;
                }
                // Small delay to ensure window is fully shown
                setTimeout(() => {
                resetToRecording();
                }, 100);
            }
            // Window just became hidden
            else if (!isVisible && wasVisible) {
                console.log('üôà Window hidden - setting to idle state');
                setState('idle');
            }

            wasVisible = isVisible;
        }, 300);  // Check more frequently (every 300ms instead of 500ms)
    </script>
</body>
</html>
